{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1cd38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import textwrap\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from data.datafinder import DataFinder\n",
    "from src.recs_metrics.item_item import recall_at_n, tndcg_at_n\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c79985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    def clean(self, text):\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove markdown artifacts\n",
    "        text = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", text)  # **bold**\n",
    "        text = re.sub(r\"\\[([^\\]]+)\\]\\([^\\)]+\\)\", r\"\\1\", text)  # markdown links [links](url)\n",
    "        text = re.sub(r\"\\[(.*?)\\]\", r\"\\1\", text)  # bare brackets\n",
    "\n",
    "        # Remove punctuation but preserve hyphenated words and IDs\n",
    "        # This preserves tokens like \"sysu-mm01-c\" or \"image-net1k\"\n",
    "        text = re.sub(r\"[^\\w\\- ]\", \" \", text)\n",
    "\n",
    "        # Remove standalone numbers (tokens that are only digits)\n",
    "        text = re.sub(r\"\\b\\d+\\b\", \" \", text)\n",
    "\n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        # Tokenize and lemmatize\n",
    "        tokens = text.split()\n",
    "        tokens = [\n",
    "            self.lemmatizer.lemmatize(t)\n",
    "            for t in tokens\n",
    "            if t not in self.stopwords and len(t) > 2\n",
    "        ]\n",
    "\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [self.clean(doc) for doc in X]\n",
    "\n",
    "# # Example\n",
    "\n",
    "# df = DataFinder()\n",
    "# corpus = df.get()[\"corpus\"]\n",
    "# cleaner = TextPreprocessor()\n",
    "\n",
    "# samples = corpus.sample(2, random_state=SEED) # sample rows\n",
    "\n",
    "# # Display original vs cleaned text\n",
    "# for _, row in samples.iterrows():\n",
    "#     raw_text = \" \".join(filter(None, [\n",
    "#         f\"{row['title']}\" if pd.notna(row['title']) else None,\n",
    "#         f\"{row['description']}\" if pd.notna(row['description']) else None,\n",
    "#         f\"{'; '.join(row['tasks'])}\" if isinstance(row['tasks'], list) else None,\n",
    "#         f\"{row['modalities']}\" if pd.notna(row['modalities']) else None\n",
    "#     ]))\n",
    "    \n",
    "#     cleaned_text = cleaner.clean(raw_text)\n",
    "\n",
    "#     print(\"Dataset ID:\", row[\"id\"])\n",
    "#     print(\"ðŸ”¹ Raw text:\\n\", textwrap.fill(raw_text, width=100))\n",
    "#     print(\"ðŸ”¸ Cleaned:\\n\", textwrap.fill(cleaned_text, width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b6b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "df = DataFinder()\n",
    "data = df.get()\n",
    "corpus = data[\"corpus\"]\n",
    "\n",
    "corpus[\"text\"] = corpus.apply(\n",
    "    lambda row: \" \".join(filter(None, [\n",
    "        f\"Title: {row['title']}.\" if pd.notna(row['title']) else None,\n",
    "        f\"Description: {row['description']}.\" if pd.notna(row['description']) else None,\n",
    "        f\"Tasks: {'; '.join(row['tasks'])}.\" if isinstance(row['tasks'], list) else None,\n",
    "        f\"Modalities: {row['modalities']}.\" if pd.notna(row['modalities']) else None\n",
    "    ])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "cleaner = TextPreprocessor()\n",
    "corpus[\"text\"] = cleaner.transform(corpus[\"text\"])\n",
    "\n",
    "# Dataset ID mappings\n",
    "id_to_idx = {id_: i for i, id_ in enumerate(corpus[\"id\"])}\n",
    "idx_to_id = {i: id_ for id_, i in id_to_idx.items()}\n",
    "\n",
    "ground_truth_links = df.get_links_from_queries()\n",
    "\n",
    "# # Split ground truth into validation and test sets\n",
    "# all_items = list(ground_truth_links.items())\n",
    "# val_items, test_items = train_test_split(all_items, test_size=0.2, random_state=SEED)\n",
    "# val_links = dict(val_items); test_links = dict(test_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b3c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "vectorizer = CountVectorizer(\n",
    "        lowercase=False, # already handled by TextPreprocessor\n",
    "        max_df=0.95,\n",
    "        min_df=5\n",
    "    ) # BoW matrix document-term\n",
    "X = vectorizer.fit_transform(corpus[\"text\"])\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=95,\n",
    "    doc_topic_prior=0.7038693060984964,\n",
    "    topic_word_prior=0.13563618275622608,\n",
    "    learning_decay=0.9000732432887424,\n",
    "    random_state=SEED\n",
    ")\n",
    "doc_topic_matrix = lda.fit_transform(X)\n",
    "similarity_matrix_lda = cosine_similarity(doc_topic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ca933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=5)\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus[\"text\"])\n",
    "similarity_matrix_tfidf = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF\n",
    "nmf = NMF(\n",
    "    n_components=95,\n",
    "    random_state=SEED,\n",
    "    init='nndsvda'\n",
    ")\n",
    "doc_topic_matrix_nmf = nmf.fit_transform(tfidf_matrix)\n",
    "similarity_matrix_nmf = cosine_similarity(doc_topic_matrix_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be56775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25\n",
    "import json\n",
    "\n",
    "datafinder_path = Path(\"development/datagems_dataset_recs/datafinder\")\n",
    "datafinder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(datafinder_path / \"bm25_input.jsonl\", \"w\") as f:\n",
    "    for _, row in corpus.iterrows():\n",
    "        json.dump({\"id\": row[\"id\"], \"contents\": row[\"text\"]}, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# # For creating the BM25 index\n",
    "# python -m pyserini.index.lucene \\\n",
    "#   --collection JsonCollection \\\n",
    "#   --input development/datagems_dataset_recs/datafinder \\\n",
    "#   --index development/datagems_dataset_recs/datafinder/bm25_index \\\n",
    "#   --generator DefaultLuceneDocumentGenerator \\\n",
    "#   --threads 2 \\\n",
    "#   --storePositions --storeDocvectors --storeRaw\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/temurin-24.jdk/Contents/Home\"\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "searcher = LuceneSearcher(str(datafinder_path / \"bm25_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # gpu if available\n",
    "\n",
    "# SciBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "model.eval()  # inference mode\n",
    "model.to(device)\n",
    "\n",
    "embeddings_scibert = []\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(corpus[\"text\"].tolist(), desc=\"Encoding with SciBERT\"):\n",
    "        inputs = tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        input_mask = inputs['attention_mask']\n",
    "        masked_embeddings = token_embeddings * input_mask.unsqueeze(-1)\n",
    "        mean_embedding = masked_embeddings.sum(1) / input_mask.sum(1).unsqueeze(-1)\n",
    "        embeddings_scibert.append(mean_embedding.squeeze().cpu().numpy())\n",
    "\n",
    "embeddings_scibert = np.array(embeddings_scibert)\n",
    "similarity_matrix_scibert = cosine_similarity(embeddings_scibert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECTER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/specter2_base\")\n",
    "model = AutoModel.from_pretrained(\"allenai/specter2_base\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "embeddings_specter = []\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(corpus[\"text\"].tolist(), desc=\"Encoding with SPECTER\"):\n",
    "        inputs = tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        input_mask = inputs['attention_mask']\n",
    "        masked_embeddings = token_embeddings * input_mask.unsqueeze(-1)\n",
    "        mean_embedding = masked_embeddings.sum(1) / input_mask.sum(1).unsqueeze(-1)\n",
    "        embeddings_specter.append(mean_embedding.squeeze().cpu().numpy())\n",
    "\n",
    "embeddings_specter = np.array(embeddings_specter)\n",
    "similarity_matrix_specter = cosine_similarity(embeddings_specter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd11680",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = {model: {} for model in [\"TF-IDF\", \"LDA\", \"NMF\", \"BM25\", \"SciBERT (pre-trained)\", \"SPECTER (pre-trained)\"]}\n",
    "\n",
    "for n in [10, 20, 50]:\n",
    "    # LDA\n",
    "    predictions_lda = {\n",
    "        idx_to_id[i]: [idx_to_id[j] for j in similarity_matrix_lda[i].argsort()[::-1] if j != i][:n]\n",
    "        for i in range(similarity_matrix_lda.shape[0])\n",
    "    }\n",
    "    recall_lda = recall_at_n(predictions_lda, ground_truth_links, n=n)\n",
    "    ndcg_lda = tndcg_at_n(predictions_lda, ground_truth_links, n=n)\n",
    "\n",
    "    # NMF\n",
    "    predictions_nmf = {\n",
    "        idx_to_id[i]: [idx_to_id[j] for j in similarity_matrix_nmf[i].argsort()[::-1] if j != i][:n]\n",
    "        for i in range(similarity_matrix_nmf.shape[0])\n",
    "    }\n",
    "    recall_nmf = recall_at_n(predictions_nmf, ground_truth_links, n=n)\n",
    "    ndcg_nmf = tndcg_at_n(predictions_nmf, ground_truth_links, n=n)\n",
    "\n",
    "    # TF-IDF\n",
    "    predictions_tfidf = {\n",
    "        idx_to_id[i]: [idx_to_id[j] for j in similarity_matrix_tfidf[i].argsort()[::-1] if j != i][:n]\n",
    "        for i in range(similarity_matrix_tfidf.shape[0])\n",
    "    }\n",
    "    recall_tfidf = recall_at_n(predictions_tfidf, ground_truth_links, n=n)\n",
    "    ndcg_tfidf = tndcg_at_n(predictions_tfidf, ground_truth_links, n=n)\n",
    "\n",
    "    # BM25\n",
    "    bm25_predictions = {}\n",
    "    for _, row in corpus.iterrows():\n",
    "        dataset_id = row[\"id\"]\n",
    "        query_text = row[\"text\"]\n",
    "        hits = searcher.search(query_text, k=60)\n",
    "        bm25_predictions[dataset_id] = [hit.docid for hit in hits if hit.docid != dataset_id][:n]\n",
    "    recall_bm25 = recall_at_n(bm25_predictions, ground_truth_links, n=n)\n",
    "    ndcg_bm25 = tndcg_at_n(bm25_predictions, ground_truth_links, n=n)\n",
    "\n",
    "    # SciBERT\n",
    "    predictions_scibert = {\n",
    "        idx_to_id[i]: [idx_to_id[j] for j in similarity_matrix_scibert[i].argsort()[::-1] if j != i][:n]\n",
    "        for i in range(len(corpus))\n",
    "    }\n",
    "    recall_scibert = recall_at_n(predictions_scibert, ground_truth_links, n=n)\n",
    "    ndcg_scibert = tndcg_at_n(predictions_scibert, ground_truth_links, n=n)\n",
    "\n",
    "    # SPECTER\n",
    "    predictions_specter = {\n",
    "        idx_to_id[i]: [idx_to_id[j] for j in similarity_matrix_specter[i].argsort()[::-1] if j != i][:n]\n",
    "        for i in range(len(corpus))\n",
    "    }\n",
    "    recall_specter = recall_at_n(predictions_specter, ground_truth_links, n=n)\n",
    "    ndcg_specter = tndcg_at_n(predictions_specter, ground_truth_links, n=n)\n",
    "\n",
    "    for model, recall, ndcg in [\n",
    "        (\"TF-IDF\", recall_tfidf, ndcg_tfidf),\n",
    "        (\"BM25\", recall_bm25, ndcg_bm25),\n",
    "        (\"LDA\", recall_lda, ndcg_lda),\n",
    "        (\"NMF\", recall_nmf, ndcg_nmf),\n",
    "        (\"SciBERT (pre-trained)\", recall_scibert, ndcg_scibert),\n",
    "        (\"SPECTER (pre-trained)\", recall_specter, ndcg_specter)\n",
    "    ]:\n",
    "        results_all[model][f\"Recall@{n}\"] = recall\n",
    "        results_all[model][f\"NDCG@{n}\"] = ndcg\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results_all, orient=\"index\")\n",
    "results_df.to_csv(datafinder_path / \"results.csv\", index_label=\"Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyserini-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
